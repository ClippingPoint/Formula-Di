{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From Alex Staravoitau\n",
    "# https://github.com/navoshta/KITTI-Dataset/blob/master/kitti-dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pykitti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = 'data/'\n",
    "\n",
    "date = '2011_09_26'\n",
    "drive = '0002'\n",
    "calib_dir = 'data/2011_09_26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(date, drive, calibrated=False, frame_range=None):\n",
    "    \"\"\"\n",
    "    Loads the dataset with `date` and `drive`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date        : Dataset creation date.\n",
    "    drive       : Dataset drive.\n",
    "    calibrated  : Flag indicating if we need to parse calibration data. Defaults to `False`.\n",
    "    frame_range : Range of frames. Defaults to `None`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Loaded dataset of type `raw`.\n",
    "    \"\"\"\n",
    "    dataset = pykitti.raw(basedir, date, drive)\n",
    "\n",
    "    # Load the data\n",
    "    if calibrated:\n",
    "        dataset.load_calib()  # Calibration data are accessible as named tuples\n",
    "    dataset.load_timestamps()  # Timestamps are parsed into datetime objects\n",
    "    dataset.load_oxts()  # OXTS packets are loaded as named tuples\n",
    "    dataset.load_gray()  # Left/right images are accessible as named tuples\n",
    "    dataset.load_rgb()  # Left/right images are accessible as named tuples\n",
    "    dataset.load_velo()  # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "#     dataset.load_\n",
    "\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    print('\\nDrive: ' + str(dataset.drive))\n",
    "    print('\\nFrame range: ' + str(dataset.frame_range))\n",
    "\n",
    "    if calibrated:\n",
    "        print('\\nIMU-to-Velodyne transformation:\\n' + str(dataset.calib.T_velo_imu))\n",
    "        print('\\nGray stereo pair baseline [m]: ' + str(dataset.calib.b_gray))\n",
    "        print('\\nRGB stereo pair baseline [m]: ' + str(dataset.calib.b_rgb))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OXTS timestamps from 2011_09_26_drive_0002_sync...\n",
      "Found 77 timestamps...\n",
      "done.\n",
      "Loading OXTS data from 2011_09_26_drive_0002_sync...\n",
      "Found 77 OXTS measurements...\n",
      "done.\n",
      "Loading monochrome images from 2011_09_26_drive_0002_sync...\n",
      "Found 77 image pairs...\n",
      "done.\n",
      "Loading color images from 2011_09_26_drive_0002_sync...\n",
      "Found 77 image pairs...\n",
      "done.\n",
      "Found 77 Velodyne scans...\n",
      "done.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'raw' object has no attribute 'load_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-16ccc16b8243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7bae95f5286e>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(date, drive, calibrated, frame_range)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Left/right images are accessible as named tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_velo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Each scan is a Nx4 array of [x,y,z,reflectance]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'raw' object has no attribute 'load_'"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(date, drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset.velo[20][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import parseTrackletXML as xmlParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_tracklets_for_frames(n_frames, xml_path):\n",
    "    \"\"\"\n",
    "    Loads dataset labels also referred to as tracklets, saving them individually for each frame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_frames    : Number of frames in the dataset.\n",
    "    xml_path    : Path to the tracklets XML.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of dictionaries with integer keys corresponding to absolute frame numbers and arrays as values. First array\n",
    "    contains coordinates of bounding box vertices for each object in the frame, and the second array contains objects\n",
    "    types as strings.\n",
    "    \"\"\"\n",
    "    tracklets = xmlParser.parseXML(xml_path)\n",
    "\n",
    "    frame_tracklets = {}\n",
    "    frame_tracklets_types = {}\n",
    "    for i in range(n_frames):\n",
    "        frame_tracklets[i] = []\n",
    "        frame_tracklets_types[i] = []\n",
    "\n",
    "    # loop over tracklets\n",
    "    for i, tracklet in enumerate(tracklets):\n",
    "        # this part is inspired by kitti object development kit matlab code: computeBox3D\n",
    "        h, w, l = tracklet.size\n",
    "        # in velodyne coordinates around zero point and without orientation yet\n",
    "        # See kitti_tracklet_box.jpg for reference\n",
    "        trackletBox = np.array([\n",
    "            [-l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2],\n",
    "            [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2],\n",
    "            [0.0, 0.0, 0.0, 0.0, h, h, h, h]\n",
    "        ])\n",
    "        # loop over all data in tracklet\n",
    "        for translation, rotation, state, occlusion, truncation, amtOcclusion, amtBorders, absoluteFrameNumber in tracklet:\n",
    "            # determine if object is in the image; otherwise continue\n",
    "            if truncation not in (xmlParser.TRUNC_IN_IMAGE, xmlParser.TRUNC_TRUNCATED):\n",
    "                continue\n",
    "            # re-create 3D bounding box in velodyne coordinate system\n",
    "            yaw = rotation[2]  # other rotations are supposedly 0\n",
    "            assert np.abs(rotation[:2]).sum() == 0, 'object rotations other than yaw given!'\n",
    "            rotMat = np.array([\n",
    "                [np.cos(yaw), -np.sin(yaw), 0.0],\n",
    "                [np.sin(yaw), np.cos(yaw), 0.0],\n",
    "                [0.0, 0.0, 1.0]\n",
    "            ])\n",
    "            cornerPosInVelo = np.dot(rotMat, trackletBox) + np.tile(translation, (8, 1)).T\n",
    "            frame_tracklets[absoluteFrameNumber] = frame_tracklets[absoluteFrameNumber] + [cornerPosInVelo]\n",
    "            frame_tracklets_types[absoluteFrameNumber] = frame_tracklets_types[absoluteFrameNumber] + [\n",
    "                tracklet.objectType]\n",
    "\n",
    "    return (frame_tracklets, frame_tracklets_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracklet_rects, tracklet_types = load_tracklets_for_frames(len(dataset.velo), 'data/{}/{}_drive_{}_sync/tracklet_labels.xml'.format(date, date, drive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.array(dataset.velo).shape)\n",
    "print(len(tracklet_rects[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'Car': 'b',\n",
    "    'Tram': 'r',\n",
    "    'Cyclist': 'g',\n",
    "    'Van': 'c',\n",
    "    'Truck': 'm',\n",
    "    'Pedestrain': 'y',\n",
    "    'Sitter': 'k'\n",
    "}\n",
    "axes_limits = [\n",
    "    [-40, 80], # X axis range\n",
    "    [-20, 20], # Y axis range\n",
    "    [-3, 10], # Z axis range\n",
    "]\n",
    "axes_str = ['X', 'Y', 'Z']\n",
    "def draw_box(pyplot_axis, vertices, axes=[0, 1, 2], color='black'):\n",
    "    \"\"\"\n",
    "    Draws a bounding 3D box in a pyplot axis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pyplot_axis : Pyplot axis to draw in.\n",
    "    vertices    : Array 8 box vertices containing x, y, z coordinates.\n",
    "    axes        : Axes to use. Defaults to `[0, 1, 2]`, e.g. x, y and z axes.\n",
    "    color       : Drawing color. Defaults to `black`.\n",
    "    \"\"\"\n",
    "    vertices = vertices[axes, :]\n",
    "    # See kitti_tracklet_box.jpg for reference\n",
    "    connections = [\n",
    "        [0, 1], [1, 2], [2, 3], [3, 0],  # Lower plane parallel to Z=0 plane\n",
    "        [4, 5], [5, 6], [6, 7], [7, 4],  # Upper plane parallel to Z=0 plane\n",
    "        [0, 4], [1, 5], [2, 6], [3, 7]  # Connections between upper and lower planes\n",
    "    ]\n",
    "    for connection in connections:\n",
    "        pyplot_axis.plot(*vertices[:, connection], c=color, lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_frame_statistics(dataset, tracklet_rects, tracklet_types, frame, points=0.2):\n",
    "    \"\"\"\n",
    "    Displays statistics for a single frame. Draws camera data, 3D plot of the lidar point cloud data and point cloud\n",
    "    projections to various planes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset         : `raw` dataset.\n",
    "    tracklet_rects  : Dictionary with tracklet bounding boxes coordinates.\n",
    "    tracklet_types  : Dictionary with tracklet types.\n",
    "    frame           : Absolute number of the frame.\n",
    "    points          : Fraction of lidar points to use. Defaults to `0.2`, e.g. 20%.\n",
    "    \"\"\"\n",
    "            \n",
    "    print('Frame timestamp: ' + str(dataset.timestamps[frame]))\n",
    "    # Draw camera data\n",
    "    f, ax = plt.subplots(2, 2, figsize=(15, 5))\n",
    "    ax[0, 0].imshow(dataset.gray[frame].left, cmap='gray')\n",
    "    ax[0, 0].set_title('Left Gray Image (cam0)')\n",
    "    ax[0, 1].imshow(dataset.gray[frame].right, cmap='gray')\n",
    "    ax[0, 1].set_title('Right Gray Image (cam1)')\n",
    "    ax[1, 0].imshow(dataset.rgb[frame].left)\n",
    "    ax[1, 0].set_title('Left RGB Image (cam2)')\n",
    "    ax[1, 1].imshow(dataset.rgb[frame].right)\n",
    "    ax[1, 1].set_title('Right RGB Image (cam3)')\n",
    "    plt.show()\n",
    "\n",
    "    points_step = int(1. / points)\n",
    "    point_size = 0.01 * (1. / points)\n",
    "    velo_range = range(0, dataset.velo[frame].shape[0], points_step)\n",
    "    velo_frame = dataset.velo[frame][velo_range, :]      \n",
    "    def draw_point_cloud(ax, title, axes=[0, 1, 2]):\n",
    "        \"\"\"\n",
    "        Convenient method for drawing various point cloud projections as a part of frame statistics.\n",
    "        \"\"\"\n",
    "        ax.scatter(*np.transpose(velo_frame[:, axes]), s=point_size, c=velo_frame[:, 3], cmap='gray')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('{} axis'.format(axes_str[axes[0]]))\n",
    "        ax.set_ylabel('{} axis'.format(axes_str[axes[1]]))\n",
    "        if len(axes) > 2:\n",
    "            ax.set_xlim3d(*axes_limits[axes[0]])\n",
    "            ax.set_ylim3d(*axes_limits[axes[1]])\n",
    "            ax.set_zlim3d(*axes_limits[axes[2]])\n",
    "            ax.set_zlabel('{} axis'.format(axes_str[axes[2]]))\n",
    "        else:\n",
    "            ax.set_xlim(*axes_limits[axes[0]])\n",
    "            ax.set_ylim(*axes_limits[axes[1]])              \n",
    "        for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n",
    "            draw_box(ax, t_rects, axes=axes, color=colors[t_type])\n",
    "            \n",
    "    # Draw point cloud data as 3D plot\n",
    "    f2 = plt.figure(figsize=(15, 8))\n",
    "    ax2 = f2.add_subplot(111, projection='3d')                    \n",
    "    draw_point_cloud(ax2, 'Velodyne scan')\n",
    "    plt.show()\n",
    "    \n",
    "    # Draw point cloud data as plane projections\n",
    "    f, ax3 = plt.subplots(3, 1, figsize=(15, 25))\n",
    "    draw_point_cloud(\n",
    "        ax3[0], \n",
    "        'Velodyne scan, XZ projection (Y = 0), the car is moving in direction left to right', \n",
    "        axes=[0, 2] # X and Z axes\n",
    "    )\n",
    "    draw_point_cloud(\n",
    "        ax3[1], \n",
    "        'Velodyne scan, XY projection (Z = 0), the car is moving in direction left to right', \n",
    "        axes=[0, 1] # X and Y axes\n",
    "    )\n",
    "    draw_point_cloud(\n",
    "        ax3[2], \n",
    "        'Velodyne scan, YZ projection (X = 0), the car is moving towards the graph plane', \n",
    "        axes=[1, 2] # Y and Z axes\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = 20\n",
    "display_frame_statistics(dataset, tracklet_rects, tracklet_types, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from source.utilities import print_progress\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from utilities import print_progress\n",
    "def draw_3d_plot(frame, dataset, tracklet_rects, tracklet_types, points=0.2):\n",
    "    \"\"\"\n",
    "    Saves a single frame for an animation: a 3D plot of the lidar data without ticks and all frame trackelts.\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame           : Absolute number of the frame.\n",
    "    dataset         : `raw` dataset.\n",
    "    tracklet_rects  : Dictionary with tracklet bounding boxes coordinates.\n",
    "    tracklet_types  : Dictionary with tracklet types.\n",
    "    points          : Fraction of lidar points to use. Defaults to `0.2`, e.g. 20%.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Saved frame filename.\n",
    "    \"\"\"\n",
    "    f = plt.figure(figsize=(12, 8))\n",
    "    axis = f.add_subplot(111, projection='3d', xticks=[], yticks=[], zticks=[])\n",
    "\n",
    "    points_step = int(1. / points)\n",
    "    point_size = 0.01 * (1. / points)\n",
    "    velo_range = range(0, dataset.velo[frame].shape[0], points_step)\n",
    "    velo_frame = dataset.velo[frame][velo_range, :]\n",
    "    axis.scatter(*np.transpose(velo_frame[:, [0, 1, 2]]), s=point_size, c=velo_frame[:, 3], cmap='gray')\n",
    "    axis.set_xlim3d(*axes_limits[0])\n",
    "    axis.set_ylim3d(*axes_limits[1])\n",
    "    axis.set_zlim3d(*axes_limits[2])\n",
    "    for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n",
    "        draw_box(axis, t_rects, axes=[0, 1, 2], color=colors[t_type])\n",
    "    filename = 'video/frame_{0:0>4}.png'.format(frame)\n",
    "    plt.savefig(filename)\n",
    "    plt.close(f)\n",
    "    return filename\n",
    "\n",
    "frames = []\n",
    "n_frames = len(dataset.velo)\n",
    "\n",
    "# print('Preparing animation frames...')\n",
    "# for i in range(n_frames):\n",
    "#     print_progress(i, n_frames - 1)\n",
    "#     filename = draw_3d_plot(i, dataset, tracklet_rects, tracklet_types)\n",
    "#     frames += [filename]\n",
    "# print('...Animation frames ready.')\n",
    "\n",
    "# clip = ImageSequenceClip(frames, fps=5)\n",
    "# % time\n",
    "# clip.write_gif('pcl_data.gif', fps=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rigid body transformation from Velodyne coordinates to camera coordinates given in calib_velo_to_cam.txt\n",
    "\n",
    "http://www.mrt.kit.edu/z/publ/download/2013/GeigerAl2013IJRR.pdf\n",
    "    \n",
    "$R^{cam}_{velo} \\in\\mathbb{R}^{3 \\times 3}$ Rotation matrix: Velodyne $\\to$ camera\n",
    "\n",
    "$t^{cam}_{velo} \\in\\mathbb{R}^{1 \\times 3}$ Translation vector: Velodyne $\\to$ camera\n",
    "\n",
    "Using: \n",
    "\n",
    "$T^{cam}_{velo} = \\begin{bmatrix}\n",
    "    R^{cam}_{velo} & t^{cam}_{velo}  \\\\\n",
    "    0 & 1\n",
    "  \\end{bmatrix}$\n",
    "  \n",
    "A 3D point $x$ in Velodyne cooridnates gets projected to a point $y$ in the $ith$ camera image as\n",
    "\n",
    "$y = P^{(i)}_{rect} R^{(0)}_{rect}T^{cam}_{velo}x$\n",
    "\n",
    "Where $P^{(i)}_{rect} = \n",
    "    \\begin{bmatrix}\n",
    "    f^{(i)}_{u} & 0 & c^{(i)}_{u} & -f^{(i)}_{u}b^{(i)}_{x} \\\\\n",
    "    0 & f^{(i)}_{v} & c^{(i)}_{v} & 0\\\\\n",
    "    0 & 0 & 1 & 0\n",
    "    \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset_utility as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calib_dir = 'data/2011_09_26/'\n",
    "cam_to_cam_filename = 'calib_cam_to_cam.txt'\n",
    "velo_to_cam_filename = 'calib_velo_to_cam.txt'\n",
    "# cam = du.loadCalibrationCamToCam(calib_dir + cam_to_cam_filename, verbose=True)\n",
    "velo_to_cam = du.loadCalibrationRigid(calib_dir + velo_to_cam_filename, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "l_and = lambda *x: np.logical_and.reduce(x)\n",
    "def convert_velo_cord_to_img(data_set, calib_dir, cam=2, frame=20):\n",
    "    \"\"\"\n",
    "    Demostrates projection of the velodyne points into the image plane\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir  : Absolute path to sequence base directory (ends with _sync)\n",
    "    calib_dir : Absolute path to directory that contains calibration files\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "#     0-based index\n",
    "#     cam = 2\n",
    "#     frame = 20\n",
    "#     load calibration\n",
    "# TODO: use os.path.join?\n",
    "    calib = du.loadCalibrationCamToCam(calib_dir + 'calib_cam_to_cam.txt')\n",
    "    Tr_velo_to_cam = du.loadCalibrationRigid(calib_dir + 'calib_velo_to_cam.txt')\n",
    "\n",
    "#     Compute projection matrix velodyne->image plane\n",
    "    R_cam_to_rect = np.eye(4, dtype=float)\n",
    "    R_cam_to_rect[0: 3, 0: 3] = calib['R_rect_00']\n",
    "    P_velo_to_img = np.dot(np.dot(calib['P_rect_0' + str(cam)], R_cam_to_rect), Tr_velo_to_cam)\n",
    "#     print(type(R_cam_to_rect))\n",
    "#     Load image and display\n",
    "#   Load velodyne points\n",
    "# Take 1 of 5 points for display speed\n",
    "    velo = data_set.velo[0:len(data_set.velo):5][0]\n",
    "#     print('data_set velo', data_set.velo[frame])\n",
    "    velo_data = data_set.velo[frame]\n",
    "    velo = velo_data[0:velo_data.shape[0]:5]\n",
    "    \n",
    "    img_h, img_w, img_ch = dataset.rgb[frame].right.shape\n",
    "    \n",
    "    img_plane_depth = 5\n",
    "    x_dir_pts = velo[:, 0]\n",
    "    filtered_x_dir_indices = l_and((x_dir_pts > img_plane_depth))\n",
    "#     .flatten to remove extra dimension\n",
    "    indices = np.argwhere(filtered_x_dir_indices).flatten()\n",
    "#     Depth (x) limited velodyne points\n",
    "    velo = velo[indices, :]\n",
    "#     Project to image plane (exclude luminance/intensity)\n",
    "    velo_img = du.project(velo[:, 0:3], P_velo_to_img)\n",
    "    \n",
    "    return velo_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_velo_to_img_size(img_shape, velo_data):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    img_size: camera image size\n",
    "    velo_data :calibrated and project transformed lidar to camera data\n",
    "    \"\"\"\n",
    "    img_h = img_shape[0]\n",
    "    img_w = img_shape[1]\n",
    "    img_dim_x_pts = velo_data[:, 0]\n",
    "    img_dim_y_pts = velo_data[:, 1]\n",
    "    \n",
    "    x_filt = l_and((img_dim_x_pts < img_w), (img_dim_x_pts >= 0))\n",
    "    y_filt = l_and((img_dim_y_pts < img_h), (img_dim_y_pts >= 0))\n",
    "    filtered = l_and(x_filt, y_filt)\n",
    "    indices = np.argwhere(filtered).flatten()\n",
    "    \n",
    "    img_dim_x_pts = img_dim_x_pts[indices]\n",
    "    img_dim_y_pts = img_dim_y_pts[indices]\n",
    "    return (img_dim_x_pts, img_dim_y_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "velo_data = convert_velo_cord_to_img(dataset, calib_dir)\n",
    "rgb_img = dataset.rgb[frame].right\n",
    "corped_velo_data = crop_velo_to_img_size(rgb_img.shape, velo_data)\n",
    "\n",
    "\n",
    "import cv2\n",
    "print(cv2.addWeighted)\n",
    "def overlay_velo_img(img, velo_data):\n",
    "    (x, y) = velo_data\n",
    "    im = np.zeros(img.shape, dtype=np.float32)\n",
    "    x_axis = np.floor(x).astype(np.int32)\n",
    "    y_axis = np.floor(y).astype(np.int32)\n",
    "#     im[y_axis, x_axis] = [1, 0, 1]\n",
    "    print(len(x))\n",
    "    print(len(y))\n",
    "    for i in range(0, len(x)):\n",
    "#         cv2.circle(img, center, radius, color, thickness=1, lineType=8, shift=0)\n",
    "        cv2.circle(img, (x_axis[i], y_axis[i]), 2, [0, 35, 0])\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(img)\n",
    "\n",
    "overlay_velo_img(rgb_img, corped_velo_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# R = run_demoVelodyne(, calib_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
